{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69faae13-ae53-4260-b005-9136480ba9d1",
   "metadata": {},
   "source": [
    "# Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1429e364-4c1f-4df5-a3c2-aa1807807eef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extração de títulos de livros\n",
    "\n",
    "- [Fonte dos dados](https://books.toscrape.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf38a8-0190-4e1f-9798-7aea891f67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "def scrape_data(url):\n",
    "    \n",
    "    # Criação dos objetos para http\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    \"\"\"Código do que você quer do site: aqui você precisa identificar os elementos HTML que contêm os dados que você deseja extrair. Vou usar um exemplo simples para extrair os títulos dos livros da página inicial.\"\"\"\n",
    "    \n",
    "    # Dados de interesse usando soup\n",
    "    titulos = soup.find_all('h3')  # Supondo que os títulos dos livros estão em tags <h3>\n",
    "\n",
    "    # Criação de uma estrutura de dados (lista, pilha ou fila) para armazenar os títulos extraídos\n",
    "    \n",
    "    titulos_livros = [title.a['title'] for title in titulos]    ## Intermediário\n",
    "\n",
    "    ## Básico\n",
    "    \"\"\"\n",
    "    for title in titles:\n",
    "        \n",
    "        book_titles.append(title.a['title'])\n",
    "\n",
    "    return book_titles\n",
    "    \"\"\"\n",
    "    \n",
    "    return titulos_livros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd3a0a-7acf-4e11-819d-857c01879b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    livros = 'https://books.toscrape.com/'\n",
    "\n",
    "    dados = scrape_data(livros)\n",
    "\n",
    "    print(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463deb73-d264-41fd-b168-3dc72c9d4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas = pd.DataFrame(dados, columns=['Títulos dos livros'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510fd92c-d18d-4acf-be3c-527f566ce1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd85d81-2492-459e-8f0c-4b5f8446a080",
   "metadata": {},
   "source": [
    "## iPhone 15 Pro\n",
    "\n",
    "Google Shopping API: A Google oferece uma API de Compras que permite aos desenvolvedores acessar informações sobre produtos, preços e disponibilidade de várias fontes online. Isso inclui informações sobre smartphones.\n",
    "\n",
    "Bing Search API: A Microsoft oferece uma API de Pesquisa que pode ser usada para obter informações sobre produtos, incluindo smartphones. A API permite realizar consultas de pesquisa e obter resultados relevantes.\n",
    "\n",
    "Wikipedia API: Embora não seja específica para compras, a API da Wikipedia pode ser usada para obter informações sobre smartphones e outros produtos. A Wikipedia muitas vezes fornece detalhes técnicos e históricos sobre dispositivos.\n",
    "\n",
    "Semantics3 API: Semantics3 é uma empresa que fornece dados de comércio eletrônico, incluindo informações sobre produtos e preços. Eles oferecem uma API que permite acessar esses dados para uma ampla variedade de produtos, incluindo smartphones.\n",
    "\n",
    "PriceAPI: PriceAPI é uma API que oferece informações de preços para uma variedade de produtos, incluindo eletrônicos e smartphones. Ela permite comparar preços de diferentes fontes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ec0b3-6020-4cbb-be79-280d4cbb81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extrair_precos(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    precos = []\n",
    "\n",
    "    # Encontre todas as tags <span> com <b>\n",
    "    spans = soup.find_all('span')\n",
    "\n",
    "    for span in spans:\n",
    "        # Encontre o texto dentro da tag <b> e remova espaços em branco\n",
    "        preco_texto = span.find('b').get_text(strip=True)\n",
    "        \n",
    "        # Adicione o preço à lista de preços\n",
    "        precos.append(preco_texto)\n",
    "\n",
    "    return precos\n",
    "\n",
    "def main():\n",
    "    url = ''  # Substitua por sua URL real\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        precos = extrair_precos(response.text)\n",
    "\n",
    "        # Imprima os preços coletados\n",
    "        for preco in precos:\n",
    "            print(f'Preço: {preco}')\n",
    "\n",
    "    else:\n",
    "        print(f'Falha ao acessar a página. Código de status: {response.status_code}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f328ba19-a3da-42d5-b15a-e9cc7802879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_author_names(url):\n",
    "    try:\n",
    "        # Obter o conteúdo HTML da página\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Verificar se a solicitação foi bem-sucedida\n",
    "\n",
    "        # Criar um objeto BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Encontrar todos os elementos <small> com a classe 'author'\n",
    "        author_elements = soup.find_all('small', class_='author')\n",
    "\n",
    "        # Extrair e imprimir os textos dos elementos\n",
    "        for author_element in author_elements:\n",
    "            author_name = author_element.text.strip()\n",
    "            print(f\"Autor: {author_name}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao fazer a solicitação: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac6a0c3-b4f5-499e-8b13-f91559629a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autor: Albert Einstein\n",
      "Autor: J.K. Rowling\n",
      "Autor: Albert Einstein\n",
      "Autor: Jane Austen\n",
      "Autor: Marilyn Monroe\n",
      "Autor: Albert Einstein\n",
      "Autor: André Gide\n",
      "Autor: Thomas A. Edison\n",
      "Autor: Eleanor Roosevelt\n",
      "Autor: Steve Martin\n"
     ]
    }
   ],
   "source": [
    "# Substitua a URL abaixo pela URL da página que deseja raspar\n",
    "url_to_scrape = 'https://quotes.toscrape.com/'\n",
    "scrape_author_names(url_to_scrape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7175874-9eca-4e02-bf33-c669aea90b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
