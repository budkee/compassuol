{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acf265e-310e-4c7f-82e3-cd6bb93f26ff",
   "metadata": {},
   "source": [
    "## Job 3 | Coleta in Batch\n",
    "\n",
    "- Passo 1: Criar o código para ler dois arquivos em `.csv` sem transformações;\n",
    "\n",
    "- [x] [script_final.py](/home/glue_user/workspace/jupyter_workspace/ingestao_batch.ipynb)\n",
    "\n",
    "### 1. Você irá executar esta coleta localmente o container docker para realizar a carga dos dados no S3 (Bucket='dados-num-lago', Arquivo='input_path').\n",
    "\n",
    "- [x] movies.csv\n",
    "- [x] series.csv\n",
    "\n",
    "### 2. Criar container Docker com um volume `-v` para armazenar os arquivos CSV e executar o processo implementado em Python.\n",
    "\n",
    "- [x] passar as credenciais ao container `glue_jupyter_lab`\n",
    "\n",
    "    `docker run -it -v /Users/camilabudke/Desenvolvimento/compass/compassuol/sprint_07/desafio_01:/home/glue_user/ -v ~/.aws:/home/glue_user/.aws -p 8888:8888 --name glue_jupyter_lab amazon/aws-glue-libs:glue_libs_4.0.0_image_01 /home/glue_user/jupyter/jupyter_start.sh`\n",
    "\n",
    "- [x] importar a lib boto3 (a imagem já tem instalado)\n",
    "- [x] ler 2 arquivos em .csv sem filtrar os dados []()\n",
    "- [x] carregar os dados para o S3 [script_final.py](lab/coleta_dados.ipynb)\n",
    "- [x] acessar a AWS Glue e gravar no S3, no bucket definido com RAW Zone\n",
    "\n",
    "\n",
    "#### No momento da gravação dos dados deve-se considerar o padrão: \n",
    "\n",
    "    S3:\\\\<nome do bucket>\\<camada de armazenamento>\\<origem do dado>\\<formato do dado>\\<especificação do dado>\\<data de processamento separada por ano\\mes\\dia>\\<arquivo>\n",
    "\n",
    "- Por exemplo:\n",
    "\n",
    "        S3:\\\\data-lake-do-fulano\\Raw\\Local\\CSV\\Movies\\2022\\05\\02\\movies.csv\n",
    "\n",
    "        S3:\\\\data-lake-do-fulano\\Raw\\Local\\CSV\\Series\\2022\\05\\02\\series.csv\n",
    "\n",
    "### Resultado esperado\n",
    "\n",
    "- Após ter coletado os dados, seu armazenamento será feito em um bucket do [Amazon S3](https://s3.console.aws.amazon.com/s3/buckets?region=us-east-1), contendo os dados brutos dos filmes e séries selecionados. \n",
    "\n",
    "![armazenamento]()\n",
    "\n",
    "### Documentações\n",
    "\n",
    "- [CSV Files | Spark](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#csv-files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51eb02a-9693-46c7-8d4b-dfe0d6d6ab45",
   "metadata": {},
   "source": [
    "## Script\n",
    "\n",
    "- Passo 1: Criar o código para ler dois arquivos em `.csv` sem transformações;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791b3ae-73b1-4914-9aa1-fa5f895304a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd69915-e315-48cb-b486-509541362ea5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelo pega nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af1d74-f2e1-404b-a87c-874d71cc4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "\n",
    "    s3_client = boto3.client('s3')\n",
    " \n",
    "    bucket_name = 'tuckbe'\n",
    "    s3_file_name = '/nomes.csv'\n",
    "    objeto = s3_client.get_object(Bucket=bucket_name, Key=s3_file_name)\n",
    "    \n",
    "    df=pandas.read_csv(objeto['Body'], sep=',')\n",
    "    rows = len(df.axes[0])\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Hello from Lambda!')\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
