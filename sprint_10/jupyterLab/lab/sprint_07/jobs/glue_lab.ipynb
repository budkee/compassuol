{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f17a98-4de8-4163-9002-b871777cda1b",
   "metadata": {},
   "source": [
    "# Glue Lab | Criando um Job\n",
    "\n",
    "- O script a seguir tem sua execução final pela plataforma da AWS, para execução em um ambiente de desenvolvimento, crie um container docker com toda instalação necessária com este comando:\n",
    "\n",
    "    `docker run -it -v ~/.aws:/home/glue_user/.aws -v $JUPYTER_WORKSPACE_LOCATION:/home/glue_user/workspace/jupyter_workspace/ -e AWS_PROFILE=default -e DISABLE_SSL=true -p 8888:8888 --name glue_jupyter_lab amazon/aws-glue-libs:glue_libs_4.0.0_image_01 /home/glue_user/jupyter/jupyter_start.sh`\n",
    "\n",
    "- Você deverá ter configurado as permissões de acesso pela plataforma e pelo terminal do docker com o comando: `aws configure`.\n",
    "\n",
    "## Links de acesso rápido\n",
    "\n",
    "- [AWS Glue | Documentação](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1dbe9-28fc-4b2f-9800-8cea9297b286",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Script Final\n",
    "\n",
    "- O script abaixo realiza a leitura de um arquivo em `.csv` (`S3_INPUT_PATH`) e retorna no próprio s3 os dados organizados por partições, passando como parâmetros as colunas por ordem de hierarquia. \n",
    "- No final, após execução na plataforma da AWS, espera-se obter o seguinte resultado:\n",
    "\n",
    "![particionamento](img/particionamento_glue.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d98374b-a05c-4c8d-8d74-268873017b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import upper\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])\n",
    "\n",
    "# Criação dos objetos\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "# Início do Job\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "\n",
    "source_file = args['S3_INPUT_PATH']\n",
    "target_path = args['S3_TARGET_PATH']\n",
    "\n",
    "# Criação do dataframe\n",
    "## Dynamic\n",
    "nomes_dy = glueContext.create_dynamic_frame.from_options(\n",
    "    \n",
    "    \"s3\",\n",
    "    {\n",
    "        \"paths\": [source_file]\n",
    "    },\n",
    "    \"csv\",\n",
    "    {\n",
    "        \"withHeader\": True, \"separator\": \",\"\n",
    "    }\n",
    "    \n",
    "    )\n",
    "\n",
    "# Transformações\n",
    "## 1. Passando o DY para DF Spark\n",
    "nomes_df = nomes_dy.toDF()\n",
    "\n",
    "# 2. Impressão do schema do DataFrame gerado\n",
    "## Dynamic\n",
    "# nomes_dy.printSchema()\n",
    "## Spark\n",
    "nomes_df.printSchema()\n",
    "\n",
    "\n",
    "# 3. Colocar os valores da coluna 'nome' em Maiúsculo\n",
    "## Dynamic\n",
    "# maiuscula = lambda nome: nome.upper() if nome is not None else None\n",
    "# nome_maiusc_col = Map.apply(frame=nomes, f = maiuscula)\n",
    "## Spark\n",
    "nome_maiusc_col = nomes_df.withColumn(\"nome\", upper(nomes_df[\"nome\"]))\n",
    "\n",
    "# Outputs\n",
    "## Dynamic\n",
    "#glueContext.write_dynamic_frame.from_options(\n",
    "#    \n",
    "#    frame = nome_maiusc_col,\n",
    "#    connection_type = \"s3\",\n",
    "#    connection_options = {\"path\": target_path, \"partitionKeys\": [\"sexo\", \"ano\"]},\n",
    "#    format = \"json\"\n",
    "#    )\n",
    "## Spark\n",
    "nome_maiusc_col.write.partitionBy(\"sexo\", \"ano\").mode('overwrite').json(target_path)\n",
    "\n",
    "# Fim do Job\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee972f-9f66-4d39-9220-21a0d4a1691b",
   "metadata": {},
   "source": [
    "### Dicas\n",
    "\n",
    "- Subir o script na plataforma e executar pelo container com os seguintes comandos:\n",
    "\n",
    "- Run: `aws glue start-job-run --job-name SEU_NOME_DE_JOB`\n",
    "- Log (json): `aws glue get-job-run --job-name MeuJobGlue --run-id SEU_ID_DE_EXECUCAO`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb7412-2349-4b61-a569-4fc54f9462c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Outras transformações "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e1c6b5-62d8-462f-b0e7-b5ae6c44d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declarações\n",
    "schema = \"nome STRING, sexo STRING, total INT, ano INT\"\n",
    "nomes = spark.read.csv(\"/home/glue_user/workspace/jupyter_workspace/dados_brutos/nomes.csv\", header=True, schema=schema)\n",
    "\n",
    "\n",
    "## Transformações \n",
    "# 1. Colocar os valores da coluna 'nome' em Maiúsculo\n",
    "# Resposta\n",
    "from pyspark.sql.functions import upper\n",
    "nome_maiusc_col = nomes.withColumn(\"nome\", upper(nomes[\"nome\"]))\n",
    "\n",
    "# 2. Contagem das linhas do DataFrame\n",
    "# Resposta\n",
    "linhas = nomes.count()\n",
    "\n",
    "# 3. Contagem de nomes agrupado por ano e sexo, e ordenado por ano de modo decrescente\n",
    "# Resposta\n",
    "from pyspark.sql.functions import count\n",
    "contagem_nomes = nomes.groupBy(\"ano\", \"sexo\").agg(count(\"nome\").alias(\"contagem\"))\n",
    "contagem_nomes_ano = contagem_nomes.orderBy(\"ano\", ascending=False)\n",
    "\n",
    "# 4. Nome feminino mais registrado e o respectivo ano de registro\n",
    "# Resposta\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "janela_analise = Window.partitionBy(\"ano\").orderBy(desc(\"total\"))\n",
    "\n",
    "nomes_fem = nomes.filter(col('sexo') == \"F\")\n",
    "nfs_mais_registrados = nomes_fem.withColumn(\"rank\", dense_rank().over(janela_analise))\n",
    "nf_mais_registrado_ano = nfs_mais_registrados.filter(nfs_mais_registrados.rank==1)\n",
    "nf_mais_registrado_final = nf_mais_registrado_ano.select(\"nome\", \"ano\", \"total\").orderBy(\"total\", ascending=False)\n",
    "\n",
    "# 5. Nome masculino mais registrado e o respectivo ano de registro\n",
    "\n",
    "# Resposta\n",
    "nomes_masc = nomes.filter(nomes.sexo == \"M\")\n",
    "nms_mais_registrados = nomes_masc.withColumn(\"rank\", dense_rank().over(janela_analise))\n",
    "nm_mais_registrado_ano = nms_mais_registrados.filter(nms_mais_registrados.rank==1)\n",
    "nm_mais_registrado_final = nm_mais_registrado_ano.select(\"nome\", \"ano\", \"total\").orderBy(\"total\", ascending=False)\n",
    "\n",
    "# 6. Total de registros para cada ano (Apresentar as 10 primeiras linhas ordenada por Ano de modo crescente)\n",
    "\n",
    "# Resposta\n",
    "registros_ano = nomes.groupBy(\"ano\").agg(sum(\"total\").alias(\"total de registros por ano\"))\n",
    "registros_ordenados = registros_ano.orderBy(\"ano\").limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed387a-c3c6-4cfe-8538-833562deac59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Modelo de Job | via AWS Glue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa5104-d576-43f2-992f-59581d79e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "JOB_NAME = 'Cola_Local'\n",
    "S3_INPUT_PATH = 'dados_brutos/nomes.csv'\n",
    "S3_TARGET_PATH = 'dados_processados/'\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])\n",
    "\n",
    "# Criação dos objetos\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "# Início do Job\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "\n",
    "source_file = args['S3_INPUT_PATH']\n",
    "target_path = args['S3_TARGET_PATH']\n",
    "\n",
    "# Criação do dataframe\n",
    "df = glueContext.create_dynamic_frame.from_options(\n",
    "    \n",
    "    \"s3\",\n",
    "    {\n",
    "        \"paths\": [source_file]\n",
    "    },\n",
    "    \"csv\",\n",
    "    {\n",
    "        \"withHeader\": True, \"separator\": \",\"\n",
    "    }\n",
    "    \n",
    ")\n",
    "\n",
    "# Transformações\n",
    "\n",
    "\n",
    "# Outputs\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    \n",
    "    frame = only_1934,\n",
    "    connection_type = \"s3\",\n",
    "    connection_options = { \"path\":target_path },\n",
    "    format = \"parquet\"\n",
    "\n",
    "    )\n",
    "\n",
    "# Fim do Job\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
