{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job 1 | Conta palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Shell | via pyspark\n",
    "\n",
    "- A contagem de palavras foi realizada com o arquivo REAMDE.md desta Sprint (7).\n",
    "\n",
    "Segue a sequência de comandos fornecidos ao pyspark dividido por etapas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "\n",
    "# Spark\n",
    "# import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('Conta Palavras').getOrCreate()\n",
    "\n",
    "# On Docker_Shell\n",
    "import re\n",
    "from pyspark.sql.functions import col, count, expr, explode, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "# arquivo = spark.read.text(\"../../README.md\")\n",
    "\n",
    "# On Docker_Shell\n",
    "arquivo = spark.read.text(\"dados_brutos/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "separa_palavras = lambda texto: re.split(r'[^\\w]+', texto)\n",
    "\n",
    "# On Shell\n",
    "## Separe as palavras por cada caractere especial encontrado, não incluindo os acentos\n",
    "df_palavras = arquivo.select(explode(split(arquivo[\"value\"], r'[^\\p{L}\\w]+')).alias(\"palavras\"))\n",
    "\n",
    "## Remova linhas em branco\n",
    "df_sem_linhas_em_branco = df_palavras.filter(col(\"palavras\").isNotNull() & (col(\"palavras\") != \"\"\n",
    "))\n",
    "\n",
    "## Contagem das palavras\n",
    "contagem_palavras = df_sem_linhas_em_branco.groupBy(\"palavras\").agg(count(\"*\").alias(\"Quantidade\"))\n",
    "\n",
    "## Ordenação decrescente pela quantidade\n",
    "df_ordenado = contagem_palavras.orderBy(col(\"Quantidade\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "| palavras|Quantidade|\n",
      "+---------+----------+\n",
      "|       da|         7|\n",
      "|        e|         5|\n",
      "|       de|         5|\n",
      "|     Glue|         3|\n",
      "|      AWS|         3|\n",
      "|   Sprint|         3|\n",
      "|       os|         3|\n",
      "|      Job|         2|\n",
      "|       se|         2|\n",
      "|estudados|         2|\n",
      "|      Lab|         2|\n",
      "|  últimas|         2|\n",
      "| entregas|         2|\n",
      "|       as|         2|\n",
      "|        a|         2|\n",
      "|  aplicar|         2|\n",
      "|  prática|         2|\n",
      "|  compass|         2|\n",
      "|    Dados|         2|\n",
      "|      nas|         2|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ordenado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Shell | via spark-submit\n",
    "\n",
    "- Contagem de palavras passando um script e o arquivo para executar o trabalho.\n",
    "\n",
    "O comando a ser executado pela linha de comando foi:\n",
    "\n",
    "    spark-submit job_conta_palavras.py -i dados_brutos/README.md -o dados_processados/conta_palavras\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    arquivo = spark.read.csv(\"\", header=True, schema=schema)\n",
    "    \n",
    "    # Job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
