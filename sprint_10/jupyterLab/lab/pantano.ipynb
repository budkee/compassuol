{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pantano de códigos\n",
    "\n",
    "- Local pra armazenar as tentativas com python, pandas e spark;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Tasks do job\n",
    "\n",
    "1. Ingestão de dados\n",
    "\n",
    "    consumo_api.py\n",
    "    \n",
    "- Entrada: endpoint\n",
    "- Transformação: agregação por coluna\n",
    "- Saída: dados_api.json\n",
    "\n",
    "2. Enviar os dados para o S3\n",
    "    \n",
    "    ingestao_batch.py\n",
    "    \n",
    "- Entrada: dados_api.json\n",
    "- Saída: URI do S3 (s3://dados-num-lago/Raw/TMDB/JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lista dos gêneros \n",
    "### de filmes\n",
    "#endpoint = \"3/genre/movie/list\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U2wnrJeJkKH",
    "tags": []
   },
   "source": [
    "### Armazenamento de dados em um DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Migrando o conteúdo do arquivo de texto para um RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler o arquivo | SparkContext (RDD)\n",
    "linhas = sc.textFile(caminho_texto)\n",
    "#linhas.collect()\n",
    "# Limpeza das linhas\n",
    "palavras = linhas.flatMap(lambda linha: linha.strip().lower.split())\n",
    "# Contagem das palavras\n",
    "contagem = palavras.map(lambda palavra: (palavra, 1))\n",
    "# Redução das quantidades\n",
    "reduzido = contagem.reduceByKey(lambda a,b: a + b)\n",
    "dados = reduzido\n",
    "colunas = [\"Palavra\", \"Quantidade\"]\n",
    "df = spark.createDataFrame(dados, colunas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Armazenamento de dados em um DataFrame | Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criar um DataFrame do Spark\n",
    "colunas = [\"Palavra\", \"Quantidade\"]\n",
    "df = spark.createDataFrame(mapeamento_palavras, colunas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI5_YExud-Zi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformações | Job 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Inicie uma sessão Spark\n",
    "spark = SparkSession.builder.appName(\"ProcessarArquivo\").getOrCreate()\n",
    "\n",
    "# Defina uma função para processar as linhas e criar um dicionário\n",
    "def processar_linha(linha):\n",
    "    mapeamento = {}\n",
    "    for palavra in [\n",
    "        re.sub(r'[^a-z]', '', unidecode(w.lower()))\n",
    "        for w in re.findall(r'\\b\\w+\\b', unidecode(linha.lower()))\n",
    "    ]:\n",
    "        if palavra:\n",
    "            mapeamento[palavra] = mapeamento.get(palavra, 0) + 1\n",
    "    return mapeamento\n",
    "\n",
    "# Registre a função como uma função UDF (User-Defined Function)\n",
    "schema = StructType([StructField(\"linha\", StringType(), True)])\n",
    "processar_linha_udf = udf(processar_linha, schema)\n",
    "\n",
    "# Carregue o arquivo de texto em um DataFrame\n",
    "caminho_texto = \"./README.md\"  # Substitua pelo caminho do seu arquivo\n",
    "df = spark.read.text(caminho_texto)\n",
    "\n",
    "# Aplique a função UDF para processar as linhas e criar uma coluna com o resultado\n",
    "df_com_mapeamento = df.withColumn(\"mapeamento\", processar_linha_udf(df[\"value\"]))\n",
    "\n",
    "# Colete o DataFrame em uma lista de dicionários\n",
    "lista_de_dicionarios = df_com_mapeamento.select(\"mapeamento\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Crie um dicionário final combinando todos os dicionários na lista\n",
    "dicionario_final = {}\n",
    "for dicionario in lista_de_dicionarios:\n",
    "    for chave, valor in dicionario.items():\n",
    "        dicionario_final[chave] = dicionario_final.get(chave, 0) + valor\n",
    "\n",
    "# Exiba o dicionário final\n",
    "print(dicionario_final)\n",
    "\n",
    "# Encerre a sessão Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Funções e Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "class WordMapper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def map(self, line):\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            print(f\"{word}\\t1\")\n",
    "\n",
    "# Exemplo de uso\n",
    "mapper = WordMapper()\n",
    "for line in sys.stdin:\n",
    "    mapper.map(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpeza das linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_linha(linha):\n",
    "    mapeamento = {}\n",
    "    for palavra in [\n",
    "        re.sub(r'[^a-z]', '', unidecode(w.lower()))\n",
    "        for w in re.findall(r'\\b\\w+\\b', unidecode(linha.lower()))\n",
    "    ]:\n",
    "        if palavra:\n",
    "            mapeamento[palavra] = mapeamento.get(palavra, 0) + 1\n",
    "    return mapeamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_arquivo(caminho):\n",
    "    try:\n",
    "        with open(caminho, \"r\") as arquivo:\n",
    "            return list(enumerate(map(str.strip, arquivo), start=1))\n",
    "    except FileNotFoundError:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Tarefa do Job | Conta palavras\n",
    "\n",
    "**Objetivo:** contar a quantidade de ocorrências das palavras em um arquivo de texto.\n",
    "\n",
    "## Passos\n",
    "\n",
    "1. Escrever uma função para mapear as palavras: WordMapperClass -> wordMapper(); \n",
    "2. Escrever uma função para reduzir a lista de frequência: WordReducerClass -> wordReducer();\n",
    "3. Escrever um script que aponte para as funções criadas e execute os métodos: WordCounterClass -> main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_SESSION\"] = sys.executable\n",
    "\n",
    "# Spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('SparkHelloWord').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: WordMapper Class\n",
    "\n",
    "- A função irá rodar uma só vez por cada linha do arquivo de texto; \n",
    "- **Input:** {\"num_da_linha\": \"Texto da linha\"}\n",
    "- **Output:** {\"palavra\": \"quantidade\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordMapper(args):\n",
    "    \n",
    "    for line in sys.stdin:\n",
    "        for word in line.split():\n",
    "            print \"%s\\t%s\" % (word,1)\n",
    "    \n",
    "    return par_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Passo 2: WordReducer Class\n",
    "\n",
    "- A função irá rodar uma só vez por cada par de chave-valor ordenado pelo Hadoop;\n",
    "\n",
    "- **Input:** {\"palavra\": \"lista_de_frequencia\"}\n",
    "- **Output:** {\"palavra\": \"contagem\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordReducer(args):\n",
    "    \n",
    "    (last_key,count) = (None, 0)\n",
    "    for line in sys.stdin:\n",
    "        (key, value) = line.strip().split(\"\\t\")\n",
    "        if last_key and last_key!=key:\n",
    "            print \"%s\\t%s\" % (last_key,count)\n",
    "            (last_key,count) = (key,int(value))\n",
    "        else:\n",
    "            (last_key,count) = (key, count + int(value))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Passo 3: Main\n",
    "\n",
    "- A função principal do programa contendo todas as configurações necessárias para realizar a tarefa;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def WordCounter(input_path, output_path):\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Objetivo:** contar a quantidade de ocorrências das palavras em um arquivo de texto.\n",
    "\n",
    "## Passos\n",
    "\n",
    "1. Escrever uma função para mapear as palavras: WordMapperClass -> wordMapper(); \n",
    "2. Escrever uma função para reduzir a lista de frequência: WordReducerClass -> wordReducer();\n",
    "3. Escrever um script que aponte para as funções criadas e execute os métodos: WordCounterClass -> main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_SESSION\"] = sys.executable\n",
    "\n",
    "# Spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('SparkHelloWord').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: WordMapper Class\n",
    "\n",
    "- A função irá rodar uma só vez por cada linha do arquivo de texto; \n",
    "- **Input:** {\"num_da_linha\": \"Texto da linha\"}\n",
    "- **Output:** {\"palavra\": \"quantidade\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordMapper(args):\n",
    "    \n",
    "    for line in sys.stdin:\n",
    "        for word in line.split():\n",
    "            print \"%s\\t%s\" % (word,1)\n",
    "    \n",
    "    return par_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Passo 2: WordReducer Class\n",
    "\n",
    "- A função irá rodar uma só vez por cada par de chave-valor ordenado pelo Hadoop;\n",
    "\n",
    "- **Input:** {\"palavra\": \"lista_de_frequencia\"}\n",
    "- **Output:** {\"palavra\": \"contagem\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordReducer(args):\n",
    "    \n",
    "    (last_key,count) = (None, 0)\n",
    "    for line in sys.stdin:\n",
    "        (key, value) = line.strip().split(\"\\t\")\n",
    "        if last_key and last_key!=key:\n",
    "            print \"%s\\t%s\" % (last_key,count)\n",
    "            (last_key,count) = (key,int(value))\n",
    "        else:\n",
    "            (last_key,count) = (key, count + int(value))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Passo 3: Main\n",
    "\n",
    "- A função principal do programa contendo todas as configurações necessárias para realizar a tarefa;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def WordCounter(input_path, output_path):\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint 6 | Athena\n",
    "\n",
    "- Crie uma consulta que lista os 3 nomes mais usados em cada década desde o 1950 até hoje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair o ano da coluna 'ano'\n",
    "df['ano'] = pd.to_numeric(df['ano'], errors='coerce')\n",
    "\n",
    "# Encontrar o intervalo de décadas com base nos anos presentes no DataFrame\n",
    "min_year = df['ano'].min()\n",
    "max_year = df['ano'].max()\n",
    "decadas_validas = range(min_year // 10 * 10, (max_year // 10 + 1) * 10, 10)\n",
    "\n",
    "# Filtrar o DataFrame para incluir apenas as décadas válidas\n",
    "df = df[df['ano'].isin(decadas_validas)]\n",
    "\n",
    "# Definir uma função para extrair a década a partir do ano\n",
    "def extrair_decada(ano):\n",
    "    return ano // 10 * 10\n",
    "\n",
    "# Nova coluna chamada década\n",
    "df['decada'] = df['ano'].apply(extrair_decada)\n",
    "\n",
    "# Agrupar por década e contar os nomes mais comuns em cada grupo\n",
    "resultado = df.groupby(['decada', 'nome'])['total'].sum().reset_index()\n",
    "\n",
    "# Ordenar o resultado pela decada e pelo total\n",
    "resultado = resultado.sort_values(by=['decada', 'total'], ascending=[True, False])\n",
    "\n",
    "# Listar os 3 nomes mais usados em cada década\n",
    "decadas = resultado['decada'].unique()\n",
    "\n",
    "for decada in decadas:\n",
    "  if decada >= 1950:\n",
    "    top_nomes_decada = resultado[resultado['decada'] == decada].head(3)\n",
    "    print(f\"Década: {decada}\")\n",
    "    print(top_nomes_decada[['nome', 'total']])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%help` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
