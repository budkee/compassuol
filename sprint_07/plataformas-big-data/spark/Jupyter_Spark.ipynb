{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WrRfqfl0MCv"
   },
   "source": [
    "# Spark & Jupyter | Ambiente de desenvolvimento e testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Objetivo:** contar a quantidade de ocorrências das palavras em um arquivo de texto.\n",
    "\n",
    "## Passos\n",
    "\n",
    "1. Escrever uma função para mapear as palavras: WordMapperClass -> wordMapper(); \n",
    "2. Escrever uma função para reduzir a lista de frequência: WordReducerClass -> wordReducer();\n",
    "3. Escrever um script que aponte para as funções criadas e execute os métodos: WordCounterClass -> main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_SESSION\"] = sys.executable\n",
    "\n",
    "# Spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('SparkHelloWord').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: WordMapper Class\n",
    "\n",
    "- A função irá rodar uma só vez por cada linha do arquivo de texto; \n",
    "- **Input:** {\"num_da_linha\": \"Texto da linha\"}\n",
    "- **Output:** {\"palavra\": \"quantidade\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordMapper(args):\n",
    "    \n",
    "    for line in sys.stdin:\n",
    "        for word in line.split():\n",
    "            print \"%s\\t%s\" % (word,1)\n",
    "    \n",
    "    return par_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Passo 2: WordReducer Class\n",
    "\n",
    "- A função irá rodar uma só vez por cada par de chave-valor ordenado pelo Hadoop;\n",
    "\n",
    "- **Input:** {\"palavra\": \"lista_de_frequencia\"}\n",
    "- **Output:** {\"palavra\": \"contagem\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordReducer(args):\n",
    "    \n",
    "    (last_key,count) = (None, 0)\n",
    "    for line in sys.stdin:\n",
    "        (key, value) = line.strip().split(\"\\t\")\n",
    "        if last_key and last_key!=key:\n",
    "            print \"%s\\t%s\" % (last_key,count)\n",
    "            (last_key,count) = (key,int(value))\n",
    "        else:\n",
    "            (last_key,count) = (key, count + int(value))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3: Main\n",
    "\n",
    "- A função principal do programa contendo todas as configurações necessárias para realizar a tarefa;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def WordCounter(input_path, output_path):\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v7adUyrx0Qii"
   },
   "outputs": [],
   "source": [
    "import os # Sistema operacional \n",
    "import sys # Funções e variáveis da shell do Python\n",
    "import getopt # Criação de opções de entrada pela CLI\n",
    "import pandas as pd # Pandas\n",
    "import pyspark # Spark \n",
    "from pyspark import SparkContext # SparkContext\n",
    "from pyspark.sql import SparkSession # SparkSession\n",
    "from pyspark.sql.functions import * # Funções SQL\n",
    "\n",
    "# SparkSession\n",
    "spark = SparkSession.builder.appName(\"NotebookTeste\").getOrCreate()\n",
    "# SparkContext\n",
    "#sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_texto = \"./README.md\"\n",
    "caminho_csv = \"./nomes.csv\"\n",
    "#nomes = spark.read.format(\"csv\").load(caminho_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job 1 | Contagem de palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura do arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_arquivo(caminho):\n",
    "    try:\n",
    "        with open(caminho, \"r\") as arquivo:\n",
    "            return list(enumerate(map(str.strip, arquivo), start=1))\n",
    "    except FileNotFoundError:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlR6k2lQc8Ar"
   },
   "source": [
    "## Transformações "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções e Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordMapper Class\n",
    "\n",
    "- A função irá rodar uma só vez por cada linha do arquivo de texto; \n",
    "- **Input:** {\"num_da_linha\": \"Texto da linha\"}\n",
    "- **Output:** {\"palavra\": \"quantidade\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "class WordMapper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def map(self, line):\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            print(f\"{word}\\t1\")\n",
    "\n",
    "# Exemplo de uso\n",
    "mapper = WordMapper()\n",
    "for line in sys.stdin:\n",
    "    mapper.map(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordReducer Class\n",
    "\n",
    "- A função irá rodar uma só vez por cada par de chave-valor ordenado pelo Hadoop;\n",
    "- **Input:** {\"palavra\": \"lista_de_frequencia\"}\n",
    "- **Output:** {\"palavra\": \"contagem\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "class WordReducer:\n",
    "    def __init__(self):\n",
    "        self.last_key = None\n",
    "        self.count = 0\n",
    "\n",
    "    def reduce(self, key, value):\n",
    "        if self.last_key and self.last_key != key:\n",
    "            print(f\"{self.last_key}\\t{self.count}\")\n",
    "            self.last_key, self.count = key, int(value)\n",
    "        else:\n",
    "            self.last_key, self.count = key, self.count + int(value)\n",
    "\n",
    "    def finalize(self):\n",
    "        if self.last_key:\n",
    "            print(f\"{self.last_key}\\t{self.count}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "reducer = WordReducer()\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split(\"\\t\")\n",
    "    reducer.reduce(key, value)\n",
    "\n",
    "reducer.finalize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpeza das linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_linha(linha):\n",
    "    mapeamento = {}\n",
    "    for palavra in [\n",
    "        re.sub(r'[^a-z]', '', unidecode(w.lower()))\n",
    "        for w in re.findall(r'\\b\\w+\\b', unidecode(linha.lower()))\n",
    "    ]:\n",
    "        if palavra:\n",
    "            mapeamento[palavra] = mapeamento.get(palavra, 0) + 1\n",
    "    return mapeamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U2wnrJeJkKH"
   },
   "source": [
    "### Armazenamento de dados em um DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Migrando o conteúdo do arquivo de texto para um RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ler o arquivo | SparkContext (RDD)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m linhas \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mtextFile(caminho_texto)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#linhas.collect()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Limpeza das linhas\u001b[39;00m\n\u001b[1;32m      5\u001b[0m palavras \u001b[38;5;241m=\u001b[39m linhas\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m linha: linha\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower\u001b[38;5;241m.\u001b[39msplit())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# Ler o arquivo | SparkContext (RDD)\n",
    "linhas = sc.textFile(caminho_texto)\n",
    "#linhas.collect()\n",
    "# Limpeza das linhas\n",
    "palavras = linhas.flatMap(lambda linha: linha.strip().lower.split())\n",
    "# Contagem das palavras\n",
    "contagem = palavras.map(lambda palavra: (palavra, 1))\n",
    "# Redução das quantidades\n",
    "reduzido = contagem.reduceByKey(lambda a,b: a + b)\n",
    "dados = reduzido\n",
    "colunas = [\"Palavra\", \"Quantidade\"]\n",
    "df = spark.createDataFrame(dados, colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "items",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:2377\u001b[0m, in \u001b[0;36mRow.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2375\u001b[0m     \u001b[38;5;66;03m# it will be slow when it has many fields,\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m     \u001b[38;5;66;03m# but this will not be used in normal cases\u001b[39;00m\n\u001b[0;32m-> 2377\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__fields__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[idx]\n",
      "\u001b[0;31mValueError\u001b[0m: 'items' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m dicionario_final \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dicionario \u001b[38;5;129;01min\u001b[39;00m lista_de_dicionarios:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chave, valor \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdicionario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m     40\u001b[0m         dicionario_final[chave] \u001b[38;5;241m=\u001b[39m dicionario_final\u001b[38;5;241m.\u001b[39mget(chave, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m valor\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Exiba o dicionário final\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:2382\u001b[0m, in \u001b[0;36mRow.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(item)\n\u001b[1;32m   2381\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m-> 2382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(item)\n",
      "\u001b[0;31mAttributeError\u001b[0m: items"
     ]
    }
   ],
   "source": [
    "# Importe as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Inicie uma sessão Spark\n",
    "spark = SparkSession.builder.appName(\"ProcessarArquivo\").getOrCreate()\n",
    "\n",
    "# Defina uma função para processar as linhas e criar um dicionário\n",
    "def processar_linha(linha):\n",
    "    mapeamento = {}\n",
    "    for palavra in [\n",
    "        re.sub(r'[^a-z]', '', unidecode(w.lower()))\n",
    "        for w in re.findall(r'\\b\\w+\\b', unidecode(linha.lower()))\n",
    "    ]:\n",
    "        if palavra:\n",
    "            mapeamento[palavra] = mapeamento.get(palavra, 0) + 1\n",
    "    return mapeamento\n",
    "\n",
    "# Registre a função como uma função UDF (User-Defined Function)\n",
    "schema = StructType([StructField(\"linha\", StringType(), True)])\n",
    "processar_linha_udf = udf(processar_linha, schema)\n",
    "\n",
    "# Carregue o arquivo de texto em um DataFrame\n",
    "caminho_texto = \"./README.md\"  # Substitua pelo caminho do seu arquivo\n",
    "df = spark.read.text(caminho_texto)\n",
    "\n",
    "# Aplique a função UDF para processar as linhas e criar uma coluna com o resultado\n",
    "df_com_mapeamento = df.withColumn(\"mapeamento\", processar_linha_udf(df[\"value\"]))\n",
    "\n",
    "# Colete o DataFrame em uma lista de dicionários\n",
    "lista_de_dicionarios = df_com_mapeamento.select(\"mapeamento\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Crie um dicionário final combinando todos os dicionários na lista\n",
    "dicionario_final = {}\n",
    "for dicionario in lista_de_dicionarios:\n",
    "    for chave, valor in dicionario.items():\n",
    "        dicionario_final[chave] = dicionario_final.get(chave, 0) + valor\n",
    "\n",
    "# Exiba o dicionário final\n",
    "print(dicionario_final)\n",
    "\n",
    "# Encerre a sessão Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Armazenamento de dados em um DataFrame | Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criar um DataFrame do Spark\n",
    "colunas = [\"Palavra\", \"Quantidade\"]\n",
    "df = spark.createDataFrame(mapeamento_palavras, colunas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Job 2 | AWS Glue\n",
    "- [] Alterar os valores da coluna `nome` para maiúsculo;\n",
    "- [] Contagem das linhas presentes no DataFrame;\n",
    "- [] Contagem de nomes agrupados por `ano` e `sexo`, ordenados por `ano` decrescente;\n",
    "- [] Nome feminino mais registrado e o respectivo `ano`;\n",
    "- [] Nome masculino mais registrado e o respectivo `ano`;\n",
    "- [] Total de registros para cada `ano`. Apresentar as 10 primeiras linhas, ordenados por `ano` crescente;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformações | Job 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI5_YExud-Zi"
   },
   "source": [
    "## Job 3 | Coleta in Batch\n",
    "\n",
    "- [] importar a lib boto3\n",
    "- [] passar as credenciais ao container `spark_jupyter`\n",
    "- [] ler 2 arquivos em .csv sem filtrar os dados\n",
    "- [] carregar os dados para o S3\n",
    "- [] acessar a AWS e grava no S3, no bucket definido com RAW Zone\n",
    "\n",
    ">      - no momento da gravação dos dados deve-se considerar o padrão: <nome do bucket>\\<camada de armazenamento>\\<origem do dado>\\<formato do dado>\\<especificação do dado>\\<data de processamento separada por ano\\mes\\dia>\\<arquivo>\n",
    ">\n",
    ">            Por exemplo:\n",
    ">\n",
    ">                   S3:\\\\data-lake-do-fulano\\Raw\\Local\\CSV\\Movies\\2022\\05\\02\\movies.csv\n",
    ">\n",
    ">                   S3:\\\\data-lake-do-fulano\\Raw\\Local\\CSV\\Series\\2022\\05\\02\\series.csv\n",
    ">\n",
    "\n",
    "- [] criar um container Docker com volume para (i) armazenar os arquivos.csv e (ii) executar o job.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformações | Job 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ny-35gAr1YXE"
   },
   "source": [
    "### Escrevendo o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "doniiSzA1-Lr",
    "outputId": "fd7ca4a8-2bd4-4d80-adc3-a4a2947a8ee9"
   },
   "outputs": [],
   "source": [
    "caminho_saida = \"./outputs\"\n",
    "resultado.write.format(\"csv\").save(caminho_saida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ruwJlqM-v51"
   },
   "source": [
    "### Finalizando a sessão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "0-36AXHjDVnx",
    "outputId": "28a1067e-db07-47ef-cc74-457f8083434e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
