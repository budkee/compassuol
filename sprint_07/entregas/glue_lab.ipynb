{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f17a98-4de8-4163-9002-b871777cda1b",
   "metadata": {},
   "source": [
    "# Job | Glue Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed387a-c3c6-4cfe-8538-833562deac59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Modelo de Produção | AWS Glue\n",
    "\n",
    "- Modelo de job adaptado aos componentes da AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7479a3c-aefd-45fe-870f-aa31f89b660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])\n",
    "\n",
    "# Criação dos objetos\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "# Início do Job\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "\n",
    "source_file = args['S3_INPUT_PATH']\n",
    "target_path = args['S3_TARGET_PATH']\n",
    "\n",
    "# Criação do dataframe\n",
    "df = glueContext.create_dynamic_frame.from_options(\n",
    "    \n",
    "    \"s3\",\n",
    "    {\n",
    "        \"paths\": [source_file]\n",
    "    },\n",
    "    \"csv\",\n",
    "    {\n",
    "        \"withHeader\": True, \"separator\": \",\"\n",
    "    }\n",
    "    \n",
    "    )\n",
    "\n",
    "# Transformações\n",
    "only_1934 = df.filter(lambda row: row['anoLancamento']=='1934')\n",
    "\n",
    "# Outputs\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    \n",
    "    frame = only_1934,\n",
    "    connection_type = \"s3\",\n",
    "    connection_options = { \"path\":target_path },\n",
    "    format = \"parquet\"\n",
    "\n",
    "    )\n",
    "\n",
    "# Fim do Job\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb7412-2349-4b61-a569-4fc54f9462c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Transformações "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e1c6b5-62d8-462f-b0e7-b5ae6c44d105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Declarações\n",
    "\n",
    "schema = \"nome STRING, sexo STRING, total INT, ano INT\"\n",
    "nomes = spark.read.csv(\"/home/glue_user/workspace/jupyter_workspace/dados_brutos/nomes.csv\", header=True, schema=schema)\n",
    "\n",
    "\n",
    "## Transformações \n",
    "# 1. Colocar os valores da coluna 'nome' em Maiúsculo\n",
    "from pyspark.sql.functions import upper\n",
    "nome_maiusc_col = nomes.withColumn(\"nome\", upper(nomes[\"nome\"]))\n",
    "\n",
    "# 2. Contagem das linhas do DataFrame\n",
    "linhas = nomes.count()\n",
    "\n",
    "# 3. Contagem de nomes agrupado por ano e sexo, e ordenado por ano de modo decrescente\n",
    "from pyspark.sql.functions import count\n",
    "contagem_nomes = nomes.groupBy(\"ano\", \"sexo\").agg(count(\"nome\").alias(\"contagem\"))\n",
    "contagem_nomes_ano = contagem_nomes.orderBy(\"ano\", ascending=False)\n",
    "\n",
    "# 4. Nome feminino mais registrado e o respectivo ano de registro\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "janela_analise = Window.partitionBy(\"ano\").orderBy(desc(\"total\"))\n",
    "\n",
    "# Criação do DataFrame de interesse\n",
    "nomes_fem = nomes.filter(col('sexo') == \"F\")\n",
    "nfs_mais_registrados = nomes_fem.withColumn(\"rank\", dense_rank().over(janela_analise))\n",
    "nf_mais_registrado_ano = nfs_mais_registrados.filter(nfs_mais_registrados.rank==1)\n",
    "nf_mais_registrado_final = nf_mais_registrado_ano.select(\"nome\", \"ano\", \"total\").orderBy(\"total\", ascending=False)\n",
    "\n",
    "# 5. Nome masculino mais registrado e o respectivo ano de registro\n",
    "\n",
    "# Criação do DataFrame de interesse\n",
    "nomes_masc = nomes.filter(nomes.sexo == \"M\")\n",
    "nms_mais_registrados = nomes_masc.withColumn(\"rank\", dense_rank().over(janela_analise))\n",
    "nm_mais_registrado_ano = nms_mais_registrados.filter(nms_mais_registrados.rank==1)\n",
    "nm_mais_registrado_final = nm_mais_registrado_ano.select(\"nome\", \"ano\", \"total\").orderBy(\"total\", ascending=False)\n",
    "\n",
    "# 6. Total de registros para cada ano (Apresentar as 10 primeiras linhas ordenada por Ano de modo crescente)\n",
    "\n",
    "# Criação do DataFrame de interesse\n",
    "registros_ano = nomes.groupBy(\"ano\").agg(sum(\"total\").alias(\"total de registros por ano\"))\n",
    "registros_ordenados = registros_ano.orderBy(\"ano\").limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b29d4",
   "metadata": {},
   "source": [
    "## Script do Job para o S3\n",
    "\n",
    "- Este script deverá ser executado na plataforma da AWS Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70354a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])\n",
    "\n",
    "# Criação dos objetos\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "# Início do Job\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# 1. Leitura do arquivo no s3\n",
    "source_file = args['S3_INPUT_PATH']\n",
    "target_path = args['S3_TARGET_PATH']\n",
    "\n",
    "# Criação do Dynamic Frame\n",
    "nomes = glueContext.create_dynamic_frame.from_options(\n",
    "    \n",
    "    \"s3\",\n",
    "    {\n",
    "        \"paths\": [source_file]\n",
    "    },\n",
    "    \"csv\",\n",
    "    {\n",
    "        \"withHeader\": True, \"separator\": \",\"\n",
    "    }\n",
    "    \n",
    ")\n",
    "\n",
    "# 2. Impressão do schema do DataFrame gerado\n",
    "nomes.schema()\n",
    "\n",
    "# 3. Colocar os valores da coluna 'nome' em Maiúsculo\n",
    "maiuscula = lambda nome: nome.upper() if nome is not None else None\n",
    "nome_maiusc_col = Map.apply(frame=nomes, f = maiuscula)\n",
    "\n",
    "# Outputs\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    \n",
    "    frame = nome_maiusc_col,\n",
    "    connection_type = \"s3\",\n",
    "    connection_options = { \"path\":target_path },\n",
    "    format = \"json\",\n",
    "    partitionKeys = [\"sexo\", \"ano\"]\n",
    "\n",
    "    )\n",
    "\n",
    "# Fim do Job\n",
    "job.commit()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
